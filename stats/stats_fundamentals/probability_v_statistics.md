In **probability**, we solve forward. It starts with an exact and well-defined rule. We use this rule to predict what will happen in the future. We don't know the results, but probability allows us to quantify what should happen.

In **statistics**, we solve backward. It starts with the end results known as data. We take this data and try to determine what well-defined rules created it.

Any statistical prediction should have some **margin of error**. That's because we are using data to estimate some unknown value.

For example, let's say we want to determine the number of fish in all of the lakes in the United States of America. To answer that question exactly, we would need to physically count all of the fish in every single lake. This is an impossible task. So we take a random smaller sample and use statistics to come up with an estimate and a margin of error.

If we insisted on a range that covers 100% of possible errors, the range is potentially unlimited. So we pick some threshold of error we're willing to tolerate (often +/- 5% or +/- 10%)

One tool for setting the threshold is assuming multiple samples would form a **normal curve** or bell curve. This distribution is extremely popular in statistics because it applies to many real-world situations. In this distribution, the most common observation is in the center which is the mean. The majority of observations are close to the middle, and as we move to more extreme values on either side, the likelihood decreases.

In statistics, we place a threshold on how unlikely our results have to be to draw a certain conclusion. This is part of **hypothesis testing**. This threshold is dependent on both judgement and context. It is also _not_ the same as the probability our conclusion is correct.