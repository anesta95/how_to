# [3.2 Exploring Census data with tidyverse tools](https://walker-data.com/census-r/wrangling-census-data-with-tidyverse-tools.html#exploring-census-data-with-tidyverse-tools)

To get started, the tidycensus and tidyverse packages are loaded. “tidyverse” is not specifically a package itself, but rather loads several core packages within the tidyverse. The package load message gives you more information

```{r}
library(tidycensus)
library(tidyverse)
```

## [3.2.1 Sorting and filtering data](https://walker-data.com/census-r/wrangling-census-data-with-tidyverse-tools.html#sorting-and-filtering-data)

Let’s request data on median age from the 2016-2020 ACS with `get_acs()` for all counties in the United States. This requires specifying `geography = "county"` and leaving state set to `NULL`, the default.

```{r}
median_age <- get_acs(
    geography = "county",
    variables = "B01002_001",
    year = 2020
)
```
A first exploratory data analysis question might involve understanding which counties are the _youngest_ and _oldest_ in the United States as measured by median age. This task can be accomplished with the `arrange()` function found in the **dplyr** package. `arrange()` sorts a dataset by values in one or more columns and returns the sorted result. To view the dataset in ascending order of a given column, supply the data object and a column name to the `arrange()` function.

```{r}
arrange(median_age, estimate)
```

To retrieve the oldest counties in the United States by median age, an analyst can use the `desc()` function available in **dplyr** to sort the estimate column in descending order.

```{r}
arrange(median_age, desc(estimate))
```

The `filter()` function in **dplyr** queries a dataset for rows where a given condition evaluates to `TRUE`, and retains those rows only. For analysts who are familiar with databases and SQL, this is equivalent to a `WHERE` clause. This helps analysts subset their data for specific areas by their characteristics, and answer questions like “how many counties in the US have a median age of 50 or older?”


```{r}
filter(median_age, estimate >= 50)
```

Functions like `arrange()` and `filter()` operate on row values and organize data by row. Other tidyverse functions, like **tidyr**’s `separate()`, operate on columns. The `NAME` column, returned by default by most **tidycensus** functions, contains a basic description of the location that can be more intuitive than the `GEOID`. For the 2016-2020 ACS, `NAME` is formatted as “X County, Y”, where X is the county name and Y is the state name. `separate()` can split this column into two columns where one retains the county name and the other retains the state; this can be useful for analysts who need to complete a comparative analysis by state.

```{r}
separate(
    median_age,
    NAME,
    into = c("county", "state"),
    sep = ", "
)
```

## [3.2.2 Using summary variables and calculating new columns](https://walker-data.com/census-r/wrangling-census-data-with-tidyverse-tools.html#using-summary-variables-and-calculating-new-columns)

Data in Census and ACS tables, as in the example above, are frequently comprised of variables that individually constitute sub-categories such as the numbers of households in different household income bands. One limitation of the approach above, however, is that the data and the resulting analysis return estimated counts, which are difficult to compare across geographies. For example, Maricopa County in Arizona is the state’s most populous county with 4.3 million residents; the second-largest county, Pima, only has just over 1 million residents and six of the state’s 15 counties have fewer than 100,000 residents. In turn, comparing Maricopa’s estimates with those of smaller counties in the state would often be inappropriate.

A solution to this issue might involve **normalizing** the estimated count data by dividing it by the overall population from which the sub-group is derived. Appropriate denominators for ACS tables are frequently found in the tables themselves as variables. In ACS table B19001, which covers the number of households by income bands, the variable `B19001_001` represents the total number of households in a given enumeration unit, which we removed from our analysis earlier. Given that this variable is an appropriate denominator for the other variables in the table, it merits its own column to facilitate the calculation of proportions or percentages.

In **tidycensus**, this can be accomplished by supplying a variable ID to the `summary_var` parameter in both the `get_acs()` and `get_decennial()` functions. When using `get_decennial()`, doing so will create two new columns for the decennial Census datasets, `summary_var` and `summary_value`, representing the summary variable ID and the summary variable’s value. When using `get_acs()`, using `summary_var` creates three new columns for the ACS datasets, `summary_var`, `summary_est`, and `summary_moe`, which include the ACS estimate and margin of error for the summary variable.

The following example uses the `summary_var` parameter to compare the population of counties in Arizona by race & Hispanic origin with their baseline populations, using data from the 2016-2020 ACS.

```{r}
race_vars <- c(
  White = "B03002_003",
  Black = "B03002_004",
  Native = "B03002_005",
  Asian = "B03002_006",
  HIPI = "B03002_007",
  Hispanic = "B03002_012"
)

az_race <- get_acs(
  geography = "county",
  state = "AZ",
  variables = race_vars,
  summary_var = "B03002_001",
  year = 2020
) 
```

By using dplyr’s `mutate()` function, we calculate a new column, `percent`, representing the percentage of each Census tract’s population that corresponds to each racial/ethnic group in 2016-2020. The `select()` function, also in dplyr, retains only those columns that we need to view.

```{r}
az_race_percent <- az_race %>%
    mutate(percent = 100 * (estimate / summary_est)) %>%
    select(NAME, variable, percent)
```

# [3.3 Group-wise Census data analysis](https://walker-data.com/census-r/wrangling-census-data-with-tidyverse-tools.html#group-wise-census-data-analysis)
The split-apply-combine model of data analysis, as discussed in Wickham ([2011](https://walker-data.com/census-r/references.html#ref-wickham2011)), is a powerful framework for analyzing demographic data. In general terms, an analyst will apply this framework as follows:

* The analyst identifies salient groups in a dataset between which they want to make comparisons. The dataset is then **split** into multiple pieces, one for each group.

* A function is then **applied** to each group in turn. This might be a simple summary function, such as taking the maximum or calculating the mean, or a custom function defined by the analyst.

* Finally, the results of the function applied to each group are **combined** back into a single dataset, allowing the analyst to compare the results by group.

In the tidyverse, split-apply-combine is implemented with the `group_by()` function in the dplyr package. `group_by()` does the work for the analyst of splitting a dataset into groups, allowing subsequent functions used by the analyst in an analytic pipeline to be applied to each group then combined back into a single dataset. The examples that follow illustrate some common group-wise analyses.

## [3.3.1 Making group-wise comparisons](https://walker-data.com/census-r/wrangling-census-data-with-tidyverse-tools.html#making-group-wise-comparisons)

The `az_race_percent` dataset created above is an example of a dataset suitable for group-wise data analysis. It includes two columns that could be used as group definitions: `NAME`, representing the county, and `variable`, representing the racial or ethnic group. Split-apply-combine could be used for either group definition to make comparisons for data in Arizona across these categories.

We can deploy group-wise data analysis to identify the largest racial or ethnic group in each county in Arizona. This involves setting up a data analysis pipeline with the **magrittr** pipe and calculating a _grouped filter_ where the `filter()` operation will be applied specific to each group. In this example, the filter condition will be specified as `percent == max(percent)`. We can read the analytic pipeline then as “Create a new dataset, `largest_group`, by using the `az_race_dataset` THEN grouping the dataset by the `NAME` column THEN filtering for rows that are equal to the maximum value of `percent` for each group.”

```{r}
largest_group <- az_race_percent %>% 
  group_by(NAME) %>% 
  filter(percent == max(percent))
```

`group_by()` is commonly paired with the `summarize()` function in data analysis pipelines. `summarize()` generates a new, condensed dataset that by default returns a column for the grouping variable(s) and columns representing the results of one or more functions applied to those groups. In the example below, the `median()` function is used to identify the median percentage for each of the racial & ethnic groups in the dataset across counties in Arizona. In turn, `variable` is passed to `group_by()` as the grouping variable.

```{r}
az_race_percent %>% 
  group_by(variable) %>% 
  summarize(median_pct = median(percent))
```

## [3.3.2 Tabulating new groups](https://walker-data.com/census-r/wrangling-census-data-with-tidyverse-tools.html#tabulating-new-groups)

Commonly, analysts will also need to calculate new custom groups to address specific analytic questions. For example, variables in ACS table B19001 represent groups of households whose household incomes fall into a variety of categories: less than \$10,000/year, between \$10,000/year and \$19,999/year, and so forth. These categories may be more granular than needed by an analyst. As such, an analyst might take the following steps: 1) recode the ACS variables into wider income bands; 2) group the data by the wider income bands; 3) calculate grouped sums to generate new estimates.

Consider the following example:

```{r}
mn_hh_income <- get_acs(
  geography = "county",
  table = "B19001",
  state = "MN",
  year = 2016
)
```

Our data include household income categories for each county in the rows. However, let’s say we only need three income categories for purposes of analysis: below \$35,000/year, between \$35,000/year and \$75,000/year, and \$75,000/year and up.


We first need to do some transformation of our data to recode the variables appropriately. First, we will remove variable `B19001_001`, which represents the total number of households for each county. Second, we use the `case_when()` function from the **dplyr** package to identify groups of variables that correspond to our desired groupings. Given that the variables are ordered in the ACS table in relationship to the household income values, the less than operator can be used to identify groups.

The syntax of `case_when()` can appear complex to beginners, so it is worth stepping through how the function works. Inside the `mutate()` function, which is used to create a new variable named `incgroup`, `case_when()` steps through a series of logical conditions that are evaluated in order similar to a series of if/else statements. The first condition is evaluated, telling the function to assign the value of `below35k` to all rows with a `variable` value that comes before `"B19001_008"` - which in this case will be `B19001_002` (income less than \$10,000) through `B19001_007` (income between \$30,000 and \$34,999). The second condition is then evaluated _for all those rows not accounted for by the first condition_. This means that `case_when()` knows not to assign `"bw35kand75k"` to the income group of $10,000 and below even though its variable comes before `B19001_013`. The final condition in `case_when()` can be set to `TRUE` which in this scenario translates as "all other values."

```{r}
mn_hh_income_recode <- mn_hh_income %>% 
  filter(variable != "B19001_001") %>% 
  mutate(incgroup = case_when(
    variable < "B19001_008" ~ "below35k",
    variable < "B19001_013" ~ "bw35kand75k",
    T ~ "above75k"
  ))
```

Our result illustrates how the different variable IDs are mapped to the new, recoded categories that we specified in `case_when()`. The `group_by() %>% summarize()` workflow can now be applied to the recoded categories by county to tabulate the data into a smaller number of groups.

```{r}
mn_group_sums <- mn_hh_income_recode %>% 
  group_by(GEOID, incgroup) %>% 
  summarize(estimate = sum(estimate))
```

Our data now reflect the new estimates by group by county.

# [3.4 Comparing ACS estimates over time](https://walker-data.com/census-r/wrangling-census-data-with-tidyverse-tools.html#comparing-acs-estimates-over-time)

A common task when working with Census data is to examine demographic change over time. 
Data from the Census API - and consequently **tidycensus** - only go back to the 2000 Decennial Census. 
For historical analysts who want to go even further back, 
decennial Census data are available since 1790 from the [National HistoricalGeographic Information System](https://www.nhgis.org/), 
or NHGIS, which will be covered in detail in Chapter 11.

## 3.4.1 Time-series analysis: some cautions

Before engaging in any sort of time series analysis of Census data, analysts need to account for potential problems that can emerge when using Census data longitudinally. One major issue that can emerge is geography changes over time. For example, let’s say we are interested in analyzing data on Oglala Lakota County, South Dakota.

```{r}
oglala_lakota_age <- get_acs(
  geography = "county",
  state = "SD",
  county = "Oglala Lakota",
  table = "B01001",
  year = 2020
)
```

To understand how the age composition of the county has changed over the past 10 years, 
we may want to look at the 2006-2010 ACS for the county. 
Normally, we would just change the year argument to `2010`:

```{r}
oglala_lakota_age_10 <- get_acs(
  geography = "county",
  state = "SD",
  county = "Oglala Lakota",
  table = "B01001",
  year = 2010
)
```

The problem here is that Oglala Lakota County had a different name in 2010, 
Shannon County, meaning that the `county = "Oglala Lakota"` argument will not 
return any data. In turn, the equivalent code for the 2006-2010 ACS would use 
`county = "Shannon"`.

```{r}
oglala_lakota_age_10 <- get_acs(
  geography = "county",
  state = "SD",
  county = "Shannon",
  table = "B01001",
  year = 2010
)
```

Note the differences in the GEOID column between the two tables of data. 
When a county or geographic entity changes its name, the Census Bureau assigns it a new GEOID, 
meaning that analysts need to take care when dealing with those changes. 
A full listing of geography changes [is available on the Census website for each year](https://www.census.gov/programs-surveys/acs/technical-documentation/table-and-geography-changes.2019.html).

In addition to changes in geographic identifiers, variable IDs can change over time as well. 
For example, the ACS Data Profile is commonly used for pre-computed normalized ACS estimates. 
Let’s say that we are interested in analyzing the percentage of residents age 25 and 
up with a 4-year college degree for counties in Colorado from the 2019 1-year ACS. 
We’d first look up the appropriate variable ID with `load_variables(2019, "acs1/profile")` then use `get_acs()`:

```{r}
co_college19 <- get_acs(
  geography = "county",
  variables = "DP02_0068P",
  state = "CO",
  survey = "acs1",
  year = 2019
)
```

We get back data for counties of population 65,000 and greater as these are the 
geographies available in the 1-year ACS. The data make sense: 
Boulder County, home to the University of Colorado, has a very high percentage 
of its population with a 4-year degree or higher. 
However, when we run the exact same query for the 2018 1-year ACS:

```{r}
co_college18 <- get_acs(
  geography = "county",
  variables = "DP02_0068P",
  state = "CO",
  survey = "acs1",
  year = 2018
)
```

The values are completely different, and clearly not percentages! 
This is because variable IDs for the Data Profile **are unique to each year** and 
in turn should not be used for time-series analysis. 
The returned results above represent the civilian population age 18 and up, 
and have nothing to do with educational attainment.

## 3.4.2 Preparing time-series ACS estimates

The safest option for time-series analysis in the ACS is to use the Comparison Profile Tables. 
These tables are available for both the 1-year and 5-year ACS, and 
allow for comparison of demographic indicators over the past five years for a given year. 
Using the Comparison Profile tables also brings the benefit of additional variable harmonization, 
such as inflation-adjusted income estimates.

Data from the Comparison Profile are accessed just like other ACS variables using `get_acs()`. 
The example below illustrates how to get data from the ACS Comparison Profile on 
inflation-adjusted median household income for counties and county-equivalents in Alaska.

```{r}
ak_income_compare <- get_acs(
  geography = "county",
  variables = c(
    income15 = "CP03_2015_062",
    income20 = "CP03_2020_062"
  ),
  state = "AK",
  year = 2020
)
```

For the 2016-2020 ACS, the “comparison year” is 2015, representing the closest 
non-overlapping 5-year dataset, which in this case is 2011-2015. 
We can examine the results, which are inflation-adjusted for appropriate comparison:

```{r}
ak_income_compare
```

### 3.4.2.1 Iterating over ACS years with tidyverse tools

Using the Detailed Tables also represents a safer option than the Data Profile, 
as it ensures that variable IDs will remain consistent across years allowing for consistent and correct analysis. 
That said, there still are some potential pitfalls to account for when using the Detailed Tables. 
The Census Bureau will add and remove variables from survey to survey depending on data needs and data availability. 
For example, questions are sometimes added and removed from the ACS survey meaning that 
you won’t always be able to get every data point for every year and geography combination. 
In turn, it is still important to check on data availability using `load_variables()` 
for the years you plan to analyze before carrying out your time-series analysis.

Let’s re-engineer the analysis above on educational attainment in Colorado counties, 
which below will be computed for a time series from 2010 to 2019. Information on 
“bachelor’s degree or higher” is split by sex and across different tiers of educational 
attainment in the detailed tables, found in ACS table 15002. Given that we only need 
a few variables (representing estimates of populations age 25+ who have finished 
a 4-year degree or graduate degrees, by sex), 
we’ll request those variables directly rather than the entire B15002 table.

```{r}
college_vars <- c("B15002_015",
                  "B15002_016",
                  "B15002_017",
                  "B15002_018",
                  "B15002_032",
                  "B15002_033",
                  "B15002_034",
                  "B15002_035")
```

We’ll now use these variables to request data on college degree holders from the ACS 
for counties in Colorado for each of the 1-year ACS surveys from 2010 to 2019. 
In most cases, this process should be streamlined with iteration. 
Thus far, we are familiar with using the year argument in `get_acs()` to request 
data for a specific year. Writing out ten different calls to `get_acs()`, 
however - one for each year - would be tedious and would require a fair amount of 
repetitive code! Iteration helps us avoid repetitive coding as it allows us to 
carry out the same process over a sequence of values. Programmers familiar with 
iteration will likely know of “loop” operators like `for` and `while`, which are available
in base R and most other programming languages in some variety. 
Base R also includes the `*apply()` family of functions (e.g. `lapply()`, `mapply()`, `sapply()`), 
which iterates over a sequence of values and applies a given function to each value.

The tidyverse approach to iteration is found in the **purrr** package. 
**purrr** includes a variety of functions that are designed to integrate well 
in workflows that require iteration and use other tidyverse tools. 
The `map_*()` family of functions iterate over values and try to return a desired result; 
`map()` returns a list, `map_int()` returns an integer vector, 
and `map_chr()` returns a character vector, for example. 
With tidycensus, the `map_dfr()` function is particularly useful. 
`map_dfr()` iterates over an input and applies it to a function or process defined by the user, 
then row-binds the result into a single data frame. 
The example below illustrates how this works for the years 2010 through 2019.

```{r}
years <- 2010:2019
names(years) <- years

# Updated with new version of purrr 1.0.0

college_by_year <- list_rbind(
  map(years, ~{
   get_acs(
     geography = "county",
     variables = college_vars,
     state = "CO",
     summary_var = "B15002_001",
     survey = "acs1",
     year = .x
   ) 
  }
  ), names_to = "year"
)
```

The `names_to` argument, which is optional but used here, creates a new column in the 
output data frame that contains values equivalent to the names of the input object, 
which in this case is years. By setting `names_to = "year"` we tell `list_rbind()` 
to name the new column that will contain these values `year`.

Let's review the result:

```{r}
college_by_year %>% 
  arrange(NAME, variable, year)
```

The result is a long-form dataset that contains a time series of each requested 
ACS variable for each county in Colorado that is available in the 1-year ACS. 
The code below outlines a `group_by() %>% summarize()` workflow for calculating 
the percentage of the population age 25 and up with a 4-year college degree, 
then uses the `pivot_wider()` function from the tidyr package to spread the years 
across the columns for tabular data display.

```{r}
college_by_year %>% 
  group_by(NAME, year) %>% 
  summarize(numerator = sum(estimate),
            denominator = first(summary_est)) %>% 
  mutate(pct_college = 100 * (numerator / denominator)) %>% 
  pivot_wider(id_cols = NAME,
              names_from = year,
              values_from = pct_college)
```

This particular format is suitable for data display or writing to an Excel 
spreadsheet for colleagues who are not R-based. Methods for visualization of 
time-series estimates from the ACS will be covered in Section [4.4](https://walker-data.com/census-r/exploring-us-census-data-with-visualization.html#visualizing-acs-estimates-over-time).

## 3.5 Handling margins of error in the American Community Survey with tidycensus

A topic of critical importance when working with data from the American Community Survey is the _margin of error_. 
As opposed to the decennial US Census, which is based on a complete enumeration of the US population, 
the ACS is based on a sample with estimates characterized by margins of error. 
By default, MOEs are returned at a 90 percent confidence level. 
This can be translated roughly as "we are 90 percent sure that the true value 
falls within a range defined by the estimate plus or minus the margin of error."

As discussed in Chapter 2, **tidycensus** takes an opinionated approach to margins of error. 
When applicable, **tidycensus** will always return the margin of error associated with an estimate, 
and does not have an option available to return estimates only. 
For "tidy" or long-form data, these margins of error will be found in the `moe` column; 
for wide-form data, margins of error will be found in columns with an `M` suffix.

The confidence level of the MOE can be controlled with the `moe_level` argument 
in `get_acs()`. The default `moe_level` is `90`, which is what the Census Bureau returns by default. 
tidycensus can also return MOEs at a confidence level of `95` or `99` which uses 
Census Bureau-recommended formulas to adjust the MOE. 
For example, we might look at data on median household income by county in 
Rhode Island using the default `moe_level` of `90`:

```{r}
get_acs(
  geography = "county",
  state = "Rhode Island",
  variables = "B19013_001",
  year = 2020
)
```

A stricter margin of error will increase the size of the MOE relative to its estimate.

```{r}
get_acs(
  geography = "county",
  state = "Rhode Island",
  variables = "B19013_001",
  year = 2020,
  moe_level = 99
)
```


## 3.5.1 Calculating derived margins of error in tidycensus

For small geographies or small populations, margins of error can get quite large, 
in some cases exceeding their corresponding estimates. In the example below, 
we can examine data on age groups by sex for the population age 65 and older for 
Census tracts in Salt Lake County, Utah. We will first generate a vector of 
variable IDs for which we want to request data from the ACS using some base R functionality.

In this workflow, an analyst has used `load_variables()` to look up the variables 
that represent estimates for populations age 65 and up; 
this includes `B01001_020` through `B01001_025` for males, 
and `B01001_044` through `B01001_049` for females. 
Typing out each variable individually would be tedious, 
so an analyst can use string concatenation to generate the required vector of variable IDs as follows:

```{r}
vars <- paste0("B01001_0", c(20:25, 44:49))
vars
```

The resulting variables object, named `vars`, can now be used to request variables in a call to `get_acs()`.

```{r}
salt_lake <- get_acs(
  geography = "tract",
  variables = vars,
  state = "Utah",
  county = "Salt Lake",
  year = 2020
)
```

We will now want to examine the margins of error around the estimates in the returned data. 
Let’s focus on a specific Census tract in Salt Lake County using `filter()`:

```{r}
example_tract <- salt_lake %>%
  filter(GEOID == "49035100100")

example_tract %>% 
  select(-NAME)
```

In many cases, the margins of error exceed their corresponding estimates. 
For example, the ACS data suggest that in Census tract `49035100100`, 
for the male population age 85 and up (variable ID `B01001_0025`), 
there are anywhere between 0 and 45 people in that Census tract. 
This can make ACS data for small geographies problematic for planning and analysis purposes.

A potential solution to large margins of error for small estimates in the ACS is 
to aggregate data upwards until a satisfactory margin of error to estimate ratio is reached. 
The US Census Bureau publishes formulas for appropriately calculating margins of 
error around such derived estimates, which are included in tidycensus with the following functions:

* `moe_sum()`: calculates a margin of error for a derived sum;
* `moe_product()`: calculates a margin of error for a derived product;
* `moe_ratio()`: calculates a margin of error for a derived ratio;
* `moe_prop()`: calculates a margin of error for a derived proportion.

In their most basic form, these functions can be used with constants. 
For example, let’s say we had an ACS estimate of 25 with a margin of error of 5 around that estimate. 
The appropriate denominator for this estimate is 100 with a margin of error of 3. 
To determine the margin of error around the derived proportion of 0.25, we can use `moe_prop()`:

```{r}
moe_prop(25, 100, 5, 3)
```

Our margin of error around the derived estimate of 0.25 (25 / 100) is approximately 0.049.

## 3.5.2 Calculating group-wise margins of error

These margin of error functions in **tidycensus** can in turn be integrated into 
tidyverse-centric analytic pipelines to handle large margins of error around estimates. 
Given that the smaller age bands in the Salt Lake City dataset are characterized by 
too much uncertainty for our analysis, we decide in this scenario to aggregate our 
data upwards to represent populations aged 65 and older by sex.

In the code below, we use the `case_when()` function to create a new column, sex, 
that represents a mapping of the variables we pulled from the ACS to their sex categories. 
We then employ a familiar `group_by() %>% summarize()` method to aggregate our data by Census 
tract and sex. Notably, the call to `summarize()` includes a call to tidycensus’s `moe_sum()` function, 
which will generate a new column that represents the margin of error around the derived sum.

```{r}
salt_lake_grouped <- salt_lake %>% 
  mutate(sex = case_when(
    str_sub(variable, start = -2) < "26" ~ "Male",
    T ~ "Female"
  )) %>% 
  group_by(GEOID, sex) %>% 
  summarize(sum_est = sum(estimate),
            sum_moe = moe_sum(moe, estimate))
```

The margins of error relative to their estimates are now much more reasonable than in the disaggregated data.

That said, the [Census Bureau issues a note of caution](https://www2.census.gov/programs-surveys/acs/tech_docs/statistical_testing/2019_Instructions_for_Stat_Testing_ACS.pdf?): 

> All derived MOE methods are approximations and users should be cautious in using them. 
This is because these methods do not consider the correlation or covariance between the basic estimates. 
They may be overestimates or underestimates of the derived estimate’s standard error 
depending on whether the two basic estimates are highly correlated in either the 
positive or negative direction. 
As a result, the approximated standard error may not match direct calculations of 
standard errors or calculations obtained through other methods.

This means that your "best bet" is to first search the ACS tables to see if your 
data are found in aggregated form elsewhere before doing the aggregation and 
MOE estimation yourself. In many cases, you’ll find aggregated information in 
the ACS combined tables, Data Profile, or Subject Tables that will 
include pre-computed margins of error for you.

